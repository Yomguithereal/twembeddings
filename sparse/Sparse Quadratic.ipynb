{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e001a-d9ad-4a85-80b9-03140f4cb361",
   "metadata": {},
   "source": [
    "# Sparse reproduction of clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a6196b-93ab-4ce7-b2c7-04f7bf073945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from datetime import datetime, timedelta\n",
    "from random import sample, choice\n",
    "from statistics import mean, median_low\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from fog.tokenizers import WordTokenizer\n",
    "from fog.metrics import sparse_normalize, sparse_dot_product\n",
    "from fog.evaluation import best_matching\n",
    "from twitwi.constants import TWEET_DATETIME_FORMAT\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b489f2-a483-4f77-8229-acb2c655707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001aa35-3247-4bfa-82d5-1e8b4c990453",
   "metadata": {},
   "source": [
    "## Constants and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59114cb3-07ff-48b8-a4c0-7dfb149bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def parse_date(created_at):\n",
    "    return datetime.strptime(created_at, TWEET_DATETIME_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f7ace-67f1-4afb-b02a-e4efc518be28",
   "metadata": {},
   "source": [
    "## Reading tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f768c4a4-8606-4b78-bf93-e0188735408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    ALL_TWEETS = list(csv.DictReader(f, delimiter='\\t'))\n",
    "    \n",
    "# Keeping tweets only once (to avoid fuzzy clusters present in the data)\n",
    "already_seen = set()\n",
    "TWEETS = []\n",
    "\n",
    "for tweet in ALL_TWEETS:\n",
    "    if tweet['id'] in already_seen:\n",
    "        continue\n",
    "    \n",
    "    already_seen.add(tweet['id'])\n",
    "    TWEETS.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263b7713-87a3-466a-9688-fa23ed49e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dates & parsing stuff\n",
    "for tweet in TWEETS:\n",
    "    tweet['event'] = int(tweet['event'])\n",
    "    tweet['date'] = parse_date(tweet['created_at'])\n",
    "    tweet['timestamp'] = tweet['date'].timestamp()\n",
    "    tweet['label'] = int(tweet['label'].split('.')[0]) if tweet['label'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984618b6-5c24-477a-9ed3-7a7aeed713a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making suuuuuuuure the tweets are sorted by date\n",
    "TWEETS = sorted(TWEETS, key=itemgetter('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce41a24-5cbf-4833-91ca-c2ff9b994463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1018722125941755905\n",
      "label_day 0.0\n",
      "event 20180716001\n",
      "text #Rennes - La sortie de prison de Djamel Beghal [Vidéo exclusive] via @letelegramme https://t.co/tbOthY1Ren\n",
      "text+quote+reply #Rennes - La sortie de prison de Djamel Beghal [Vidéo exclusive] via @letelegramme https://t.co/tbOthY1Ren  \n",
      "image \n",
      "url_image \n",
      "user1 True\n",
      "user2 True\n",
      "user3 True\n",
      "created_at Mon Jul 16 05:00:56 +0000 2018\n",
      "label 0\n",
      "date 2018-07-16 05:00:56\n",
      "timestamp 1531710056.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in TWEETS[0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfeef6e-7d15-41b6-ac37-53dafdf13ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 137757\n",
      "Total number of events: 327\n",
      "Total number of labels: 257\n",
      "Total number of tweets not labeled 41961\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets:', len(TWEETS))\n",
    "print('Total number of events:', len(set(t['event'] for t in TWEETS)))\n",
    "print('Total number of labels:', len(set(t['label'] for t in TWEETS if t['label'] is not None)))\n",
    "print('Total number of tweets not labeled', sum(1 if t['label'] is None else 0 for t in TWEETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357cfbfa-1562-46e8-ac54-5f659a88ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTH = defaultdict(list)\n",
    "\n",
    "for tweet in TWEETS:\n",
    "    if tweet['label'] is None:\n",
    "        continue\n",
    "        \n",
    "    TRUTH[tweet['label']].append(tweet['id'])\n",
    "\n",
    "TRUTH = list(TRUTH.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9ae949-8030-4c2d-b5b0-63db36f45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_stats(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print('Number of clusters:', len(clusters))\n",
    "    print('Max number of tweets in clusters:', max(lens))\n",
    "    print('Min number of tweets in clusters:', min(lens))\n",
    "    print('Mean number of tweets in clusters:', mean(lens))\n",
    "    print('Median number of tweets in clusters:', median_low(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbac5d84-85f5-4d25-a16d-7a9dd636af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098701-1e48-4de1-9503-7b578bb80473",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "*NOTE: It might be useful to convert tokens to incremental ids to speed up hash computations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64233d86-0760-4bc2-9558-d70d87fade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "    keep=['word'],\n",
    "    lower=True,\n",
    "    unidecode=True,\n",
    "    split_hashtags=True,\n",
    "    stoplist=STOP_WORDS_FR + [t + \"'\" for t in STOP_WORDS_FR],\n",
    "    reduce_words=True,\n",
    "    decode_html_entities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c23b5b-ae46-42b2-8442-2093e966575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ne meurent que ceux que l'on oublie. RIP Adama. Le peuple réclame VÉRITÉ ET JUSTICE ! Les coupables doivent payer !!! #MarcheAdama https://t.co/4ek07xKYwN\n",
      "[('word', 'meurent'), ('word', 'oublie'), ('word', 'rip'), ('word', 'adama'), ('word', 'peuple'), ('word', 'reclame'), ('word', 'verite'), ('word', 'justice'), ('word', 'coupables'), ('word', 'doivent'), ('word', 'payer'), ('word', 'marche'), ('word', 'adama')]\n",
      "\n",
      "TRAILER : PARIS SAINT-GERMAIN vs MONACO https://t.co/GMERosYDYP #PSG https://t.co/VyEulbDrXT\n",
      "[('word', 'trailer'), ('word', 'paris'), ('word', 'saint-germain'), ('word', 'monaco'), ('word', 'psg')]\n",
      "\n",
      "Sport: Blessé dans une chute, Vincenzo Nibali abandonne&gt; Fin en queue d =&gt; https://t.co/Y9yRNa81N2 https://t.co/BKVjUK9zhf\n",
      "[('word', 'sport'), ('word', 'blesse'), ('word', 'chute'), ('word', 'vincenzo'), ('word', 'nibali'), ('word', 'abandonne'), ('word', 'fin'), ('word', 'queue')]\n",
      "\n",
      "Etats-Unis: le gouvernement peine à réunir les familles de migrants séparées https://t.co/U4K5Mgh0cR via @RFI\n",
      "[('word', 'etats-unis'), ('word', 'gouvernement'), ('word', 'peine'), ('word', 'reunir'), ('word', 'familles'), ('word', 'migrants'), ('word', 'separees')]\n",
      "\n",
      "Paris: Fin de la grève à la tour Eiffel https://t.co/f4idrtLwUj https://t.co/lT4D8HfM2u\n",
      "[('word', 'paris'), ('word', 'fin'), ('word', 'greve'), ('word', 'tour'), ('word', 'eiffel')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_to_tokenize = sample(TWEETS, 5)\n",
    "\n",
    "for tweet in sample_to_tokenize:\n",
    "    print(tweet['text'])\n",
    "    print(list(tokenizer(tweet['text'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cbcfd1-3dcf-4138-a07d-ba386395893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8298d9ecfc5d4b5bbf437154c60ae171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCIES = Counter()\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    tweet['tokens'] = set(token for _, token in tokenizer(tweet['text']))\n",
    "    for token in tweet['tokens']:\n",
    "        DOCUMENT_FREQUENCIES[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e87b55-0fe8-4050-a2c1-ea8708ba85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 84347\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary:', len(DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00369b8-515f-437c-9e53-cd64e97ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(TWEETS)\n",
    "TOKEN_IDS = {}\n",
    "INVERSE_DOCUMENT_FREQUENCIES = {}\n",
    "\n",
    "for i, (token, df) in enumerate(DOCUMENT_FREQUENCIES.items()):\n",
    "    if df < 10:\n",
    "        continue\n",
    "    TOKEN_IDS[token] = i\n",
    "    INVERSE_DOCUMENT_FREQUENCIES[token] = 1 + math.log((N + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbae9d4f-1b8e-4382-86fa-4b869e3958b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary after df trimming: 14225\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary after df trimming:', len(INVERSE_DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3bb566a-486f-46e8-843a-caf57ae39435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd534b1ce6f74e77bd0c817c8be9f73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTORS: List[Dict[int, float]] = []\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    vector = {}\n",
    "\n",
    "    for token in tweet['tokens']:\n",
    "        idf = INVERSE_DOCUMENT_FREQUENCIES.get(token)\n",
    "        \n",
    "        if idf is None:\n",
    "            continue\n",
    "        \n",
    "        vector[TOKEN_IDS[token]] = INVERSE_DOCUMENT_FREQUENCIES[token]\n",
    "        \n",
    "    # TODO: I need to make fog's sparse_normalize mutating\n",
    "    vector = sparse_normalize(vector)\n",
    "    VECTORS.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52584103-a605-4688-98b6-af7e5641e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{372: 0.2766398855213029,\n",
       " 1054: 0.34876955102946977,\n",
       " 1055: 0.27788742214408585,\n",
       " 1056: 0.34202783907661105,\n",
       " 1057: 0.2771693987582258,\n",
       " 313: 0.3115883262780189,\n",
       " 1058: 0.2819043802989637,\n",
       " 604: 0.22127755616987577,\n",
       " 538: 0.19204439363118028,\n",
       " 1059: 0.30279993150730905,\n",
       " 1060: 0.264123107796558,\n",
       " 1061: 0.3270349385448918}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS[254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e3cb8e-cac6-40b7-aa2e-4f7c5a23e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997118113780062"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 if v else 0 for v in VECTORS) / len(VECTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003abe-493c-45d7-bed4-3b8ddb550dc1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7450cda3-57bd-4e82-ae22-d386ce1df103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import cython\n",
    "from datetime import timedelta\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def sparse_dot_product(A: dict, B: dict):\n",
    "    \n",
    "    # Swapping so we iterate over the smallest set\n",
    "    if len(A) > len(B):\n",
    "        A, B = B, A\n",
    "\n",
    "    cdef float product  = 0.0\n",
    "\n",
    "    for k, w1 in A.items():\n",
    "        w2 = B.get(k)\n",
    "\n",
    "        if w2 is not None:\n",
    "            product += w1 * w2\n",
    "\n",
    "    return product\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def clustering(vectors: list, tweets: list):\n",
    "    cdef float best_metric\n",
    "    cdef int best_candidate\n",
    "    cdef float d\n",
    "    \n",
    "    cdef float one_day = timedelta(days=1).total_seconds()\n",
    "    cdef float date_bound\n",
    "    \n",
    "    for i, A in enumerate(vectors):\n",
    "        if not A:\n",
    "            continue\n",
    "        \n",
    "        tweet_a = tweets[i]\n",
    "\n",
    "        date_bound = tweet_a['timestamp'] + one_day\n",
    "        best_metric = -1.0\n",
    "        best_candidate = -1\n",
    "        d = 0.0\n",
    "\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            tweet_b = tweets[j]\n",
    "\n",
    "            if tweet_b['timestamp'] > date_bound:\n",
    "                break\n",
    "\n",
    "            B = vectors[j]\n",
    "            \n",
    "            if not B:\n",
    "                continue\n",
    "\n",
    "            d = sparse_dot_product(A, B)\n",
    "\n",
    "            if d == 0.0:\n",
    "                continue\n",
    "\n",
    "            if best_candidate < 0 or d > best_metric:\n",
    "                best_metric = d\n",
    "                best_candidate = j\n",
    "        \n",
    "        if best_candidate != -1:\n",
    "            yield (i, best_candidate, best_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239f848a-fd52-485e-bb22-becfe33b1e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcb302c4169414bb0802853f31049ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EDGES = []\n",
    "\n",
    "for edge in tqdm(clustering(VECTORS, TWEETS), unit='tweet', total=len(VECTORS)):\n",
    "    EDGES.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c19bc2e4-a349-4f9e-8dde-fe09a634e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar tweets (similarity: 0.540190):\n",
      "\n",
      "1022091464023711745 - Si tu m'crois pas hé, Tar' ta gueule à la récrén #quilsViennentMeChercher #Benalla #AffaireMacronBenalla #AlainSouchon https://t.co/myFCDzjw20\n",
      "---\n",
      "1022140895125274624 - Accablant #AllonsChercherMacron #quilsViennentMeChercher #AffaireBenallaMacron https://t.co/o8RKTOyvrv\n"
     ]
    }
   ],
   "source": [
    "similar_pair = choice(EDGES)\n",
    "first_tweet = TWEETS[similar_pair[0]]\n",
    "second_tweet = TWEETS[similar_pair[1]]\n",
    "print('Similar tweets (similarity: %f):\\n' % similar_pair[2])\n",
    "print(first_tweet['id'], '-', first_tweet['text'])\n",
    "print('---')\n",
    "print(second_tweet['id'], '-', second_tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b553fe-1ac4-4320-a6e5-6f08da779882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def components(t):\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    for i, j, sim in EDGES:\n",
    "        i_id = TWEETS[i]['id']\n",
    "        j_id = TWEETS[j]['id']\n",
    "\n",
    "        graph.add_node(i_id)\n",
    "        graph.add_node(j_id)\n",
    "\n",
    "        if sim < t:\n",
    "            continue\n",
    "\n",
    "        graph.add_edge(i_id, j_id)\n",
    "\n",
    "    return list(nx.connected_components(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6cecd-f38e-476f-ab8b-05a064722c60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f03141f-457b-4b8e-9b99-ee9523ee84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9953ef19-c921-4fac-9ae6-740cd2ddf7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0.01\n",
      "Number of clusters: 84\n",
      "Max number of tweets in clusters: 69733\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 1634.5119047619048\n",
      "Median number of tweets in clusters: 3\n",
      "(0.7722777423032048, 0.12360679458401225, 0.0998224351198624)\n",
      "\n",
      "t = 0.05\n",
      "Number of clusters: 87\n",
      "Max number of tweets in clusters: 63874\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 1578.1494252873563\n",
      "Median number of tweets in clusters: 3\n",
      "(0.7684391823178585, 0.13028627637926346, 0.1021785318103)\n",
      "\n",
      "t = 0.1\n",
      "Number of clusters: 106\n",
      "Max number of tweets in clusters: 61918\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 1295.2735849056603\n",
      "Median number of tweets in clusters: 3\n",
      "(0.7731104910580578, 0.13513533115515372, 0.1119553283994552)\n",
      "\n",
      "t = 0.135\n",
      "Number of clusters: 300\n",
      "Max number of tweets in clusters: 53775\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 457.66333333333336\n",
      "Median number of tweets in clusters: 2\n",
      "(0.8398624460543302, 0.1462900857273107, 0.1284114048399506)\n",
      "\n",
      "t = 0.15\n",
      "Number of clusters: 694\n",
      "Max number of tweets in clusters: 51659\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 197.8371757925072\n",
      "Median number of tweets in clusters: 1\n",
      "(0.8981554992211606, 0.11682720469670232, 0.11152870018586002)\n",
      "\n",
      "t = 0.2\n",
      "Number of clusters: 5125\n",
      "Max number of tweets in clusters: 34586\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 26.790048780487805\n",
      "Median number of tweets in clusters: 1\n",
      "(0.9628317431061676, 0.04460499783451519, 0.04832364306162693)\n",
      "\n",
      "t = 0.7\n",
      "Number of clusters: 104981\n",
      "Max number of tweets in clusters: 1026\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 1.3078461816900202\n",
      "Median number of tweets in clusters: 1\n",
      "(0.999528942545971, 0.0036007250711045905, 0.006190370641159336)\n",
      "\n",
      "t = 0.8\n",
      "Number of clusters: 112472\n",
      "Max number of tweets in clusters: 1026\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 1.2207393840244682\n",
      "Median number of tweets in clusters: 1\n",
      "(0.9997766578027614, 0.0033242061796661176, 0.00584870028865285)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in (0.01, 0.05, 0.1, 0.135, 0.15, 0.2, 0.7, 0.8):\n",
    "    print('t =', t)\n",
    "    clusters = components(t)\n",
    "    print_cluster_stats(clusters)\n",
    "    print(best_matching(TRUTH, clusters, allow_additional_items=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb8cb0-137d-44b2-ae52-50dec0fd6c74",
   "metadata": {},
   "source": [
    "## Sanity tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d7ad7d-7a6e-48c7-b327-8a0b80979461",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_INDEX = {}\n",
    "\n",
    "for i, t in enumerate(TWEETS):\n",
    "    t['index'] = i\n",
    "    TWEETS_INDEX[t['id']] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4734448-21e8-4723-8361-bd744b431ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8370100855827332,\n",
       " 0.8004685044288635,\n",
       " 0.8370100855827332,\n",
       " 0.7113387584686279,\n",
       " 0.7549543380737305,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906,\n",
       " 0.7799415588378906]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = []\n",
    "\n",
    "t1_id = TRUTH[0][47]\n",
    "t1 = TWEETS_INDEX[t1_id]\n",
    "v1 = VECTORS[t1['index']]\n",
    "\n",
    "for t2_id in TRUTH[0]:\n",
    "    if t1_id == t2_id:\n",
    "        continue\n",
    "        \n",
    "    t2 = TWEETS_INDEX[t2_id]\n",
    "    v2 = VECTORS[t2['index']]\n",
    "    \n",
    "    sims.append(sparse_dot_product(v1, v2))\n",
    "\n",
    "[s for s in sims if s > 0.7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
