{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e001a-d9ad-4a85-80b9-03140f4cb361",
   "metadata": {},
   "source": [
    "# Sparse reproduction of clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a6196b-93ab-4ce7-b2c7-04f7bf073945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from datetime import datetime, timedelta\n",
    "from random import sample, choice\n",
    "from statistics import mean, median_low\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from fog.tokenizers import WordTokenizer\n",
    "from fog.metrics import sparse_normalize, sparse_dot_product\n",
    "from fog.evaluation import best_matching\n",
    "from twitwi.constants import TWEET_DATETIME_FORMAT\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001aa35-3247-4bfa-82d5-1e8b4c990453",
   "metadata": {},
   "source": [
    "## Constants and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59114cb3-07ff-48b8-a4c0-7dfb149bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def parse_date(created_at):\n",
    "    return datetime.strptime(created_at, TWEET_DATETIME_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f7ace-67f1-4afb-b02a-e4efc518be28",
   "metadata": {},
   "source": [
    "## Reading tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f768c4a4-8606-4b78-bf93-e0188735408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    ALL_TWEETS = list(csv.DictReader(f, delimiter='\\t'))\n",
    "    \n",
    "# Keeping tweets only once (to avoid fuzzy clusters present in the data)\n",
    "already_seen = set()\n",
    "TWEETS = []\n",
    "\n",
    "for tweet in ALL_TWEETS:\n",
    "    if tweet['id'] in already_seen:\n",
    "        continue\n",
    "    \n",
    "    already_seen.add(tweet['id'])\n",
    "    TWEETS.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263b7713-87a3-466a-9688-fa23ed49e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dates & parsing stuff\n",
    "for tweet in TWEETS:\n",
    "    tweet['event'] = int(tweet['event'])\n",
    "    tweet['date'] = parse_date(tweet['created_at'])\n",
    "    tweet['label'] = int(tweet['label'].split('.')[0]) if tweet['label'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984618b6-5c24-477a-9ed3-7a7aeed713a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making suuuuuuuure the tweets are sorted by date\n",
    "TWEETS = sorted(TWEETS, key=itemgetter('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce41a24-5cbf-4833-91ca-c2ff9b994463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1018722125941755905\n",
      "label_day 0.0\n",
      "event 20180716001\n",
      "text #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren\n",
      "text+quote+reply #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren  \n",
      "image \n",
      "url_image \n",
      "user1 True\n",
      "user2 True\n",
      "user3 True\n",
      "created_at Mon Jul 16 05:00:56 +0000 2018\n",
      "label 0\n",
      "date 2018-07-16 05:00:56\n"
     ]
    }
   ],
   "source": [
    "for k, v in TWEETS[0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfeef6e-7d15-41b6-ac37-53dafdf13ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 137757\n",
      "Total number of events: 327\n",
      "Total number of labels: 257\n",
      "Total number of tweets not labeled 41961\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets:', len(TWEETS))\n",
    "print('Total number of events:', len(set(t['event'] for t in TWEETS)))\n",
    "print('Total number of labels:', len(set(t['label'] for t in TWEETS if t['label'] is not None)))\n",
    "print('Total number of tweets not labeled', sum(1 if t['label'] is None else 0 for t in TWEETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357cfbfa-1562-46e8-ac54-5f659a88ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTH = defaultdict(list)\n",
    "\n",
    "for tweet in TWEETS:\n",
    "    if tweet['label'] is None:\n",
    "        continue\n",
    "        \n",
    "    TRUTH[tweet['label']].append(tweet['id'])\n",
    "\n",
    "TRUTH = list(TRUTH.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9ae949-8030-4c2d-b5b0-63db36f45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_stats(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print('Number of clusters:', len(clusters))\n",
    "    print('Max number of tweets in labeled clusters:', max(lens))\n",
    "    print('Min number of tweets in labeled clusters:', min(lens))\n",
    "    print('Mean number of tweets in labeled clusters:', mean(lens))\n",
    "    print('Median number of tweets in labeled clusters:', median_low(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbac5d84-85f5-4d25-a16d-7a9dd636af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in labeled clusters: 18020\n",
      "Min number of tweets in labeled clusters: 2\n",
      "Mean number of tweets in labeled clusters: 372.74708171206225\n",
      "Median number of tweets in labeled clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098701-1e48-4de1-9503-7b578bb80473",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "*NOTE: It might be useful to convert tokens to incremental ids to speed up hash computations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64233d86-0760-4bc2-9558-d70d87fade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "    keep=['word'],\n",
    "    lower=True,\n",
    "    unidecode=True,\n",
    "    split_hashtags=True,\n",
    "    stoplist=STOP_WORDS_FR,\n",
    "    reduce_words=True,\n",
    "    decode_html_entities=True,\n",
    "    min_word_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c23b5b-ae46-42b2-8442-2093e966575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et bien sur je n‚Äôoublie pas #NgoloKante #AdilRami #HugoLloris #thomasLemar #stevenNzonzi  et le bosse #didierdeschamps ainsi que tout le staff @equipedefrance  @SteveMandanda @FlorianThauvin @AntoGriezmann @benmendy23 @raphaelvarane @samumtiti @_OlivierGiroud_ @BenPavard28 @paulpogba @LucasHernandez @NabilFekir @kimpembe_3 @CorentinTolisso @MATUIDIBlaise @KMbappe @DjibrilSidibeS3 @Dembouz @AreolaOfficiel  merci pour tout ce bonheur !\n",
      "[('word', 'oublie'), ('word', 'ngolo'), ('word', 'kante'), ('word', 'adil'), ('word', 'rami'), ('word', 'hugo'), ('word', 'lloris'), ('word', 'thomas'), ('word', 'lemar'), ('word', 'steven'), ('word', 'nzonzi'), ('word', 'bosse'), ('word', 'didierdeschamps'), ('word', 'ainsi'), ('word', 'staff'), ('word', 'bonheur')]\n",
      "\n",
      "Gr√®ce. √Ä cause de sa politique migratoire appuy√©e par l‚ÄôUnion europ√©enne qui bloque des milliers d‚Äôenfants demandeurs d‚Äôasile dans les √Æles de la mer √âg√©e, la Gr√®ce prive  ces enfants de leur droit √† l‚Äô√©ducation, a d√©clar√© Human Rights Watch aujourd‚Äôhui! https://t.co/Wu7VVfBJdy  \n",
      "[('word', 'grece'), ('word', 'cause'), ('word', 'politique'), ('word', 'migratoire'), ('word', 'appuyee'), ('word', 'union'), ('word', 'europeenne'), ('word', 'bloque'), ('word', 'milliers'), ('word', 'enfants'), ('word', 'demandeurs'), ('word', 'asile'), ('word', 'iles'), ('word', 'mer'), ('word', 'egee'), ('word', 'grece'), ('word', 'prive'), ('word', 'enfants'), ('word', 'droit'), ('word', 'education'), ('word', 'declare'), ('word', 'human'), ('word', 'rights'), ('word', 'watch'), ('word', \"aujourd'hui\")]\n",
      "\n",
      "More important analysis  https://t.co/BvKkDAPsgU  \n",
      "[('word', 'more'), ('word', 'important'), ('word', 'analysis')]\n",
      "\n",
      "#TDF2018 Premi√®re attaque d'un favori merci Dan Martin. Mais \"enfin\" comme dirait Alexandre Pasteur  \n",
      "[('word', 'f2018'), ('word', 'premiere'), ('word', 'attaque'), ('word', 'favori'), ('word', 'dan'), ('word', 'martin'), ('word', 'enfin'), ('word', 'dirait'), ('word', 'alexandre'), ('word', 'pasteur')]\n",
      "\n",
      "@berbinch40 @khalucile @MannyKoshka Ouais le fond est pas https://t.co/Ylk7pkXjP5 m√®re m'a dit \"t'es s√©rieuse tu aurais pu cach√© le derriere\"üò≠üò≠  \n",
      "[('word', 'fond'), ('word', 'mere'), ('word', 'serieuse'), ('word', 'aurais'), ('word', 'cache'), ('word', 'derriere')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_to_tokenize = sample(TWEETS, 5)\n",
    "\n",
    "for tweet in sample_to_tokenize:\n",
    "    print(tweet['text+quote+reply'])\n",
    "    print(list(tokenizer(tweet['text+quote+reply'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41cbcfd1-3dcf-4138-a07d-ba386395893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3009fc31f4a11947c92d34f0efcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCIES = Counter()\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    tweet['tokens'] = [token for _, token in tokenizer(tweet['text+quote+reply'])]\n",
    "    for token in tweet['tokens']:\n",
    "        DOCUMENT_FREQUENCIES[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75e87b55-0fe8-4050-a2c1-ea8708ba85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 94789\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary:', len(DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00369b8-515f-437c-9e53-cd64e97ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(DOCUMENT_FREQUENCIES)\n",
    "TOKEN_IDS = {}\n",
    "INVERSE_DOCUMENT_FREQUENCIES = {}\n",
    "\n",
    "for i, (token, df) in enumerate(DOCUMENT_FREQUENCIES.items()):\n",
    "    TOKEN_IDS[token] = i\n",
    "    INVERSE_DOCUMENT_FREQUENCIES[token] = math.log(N / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3bb566a-486f-46e8-843a-caf57ae39435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec18c8bc36a4740a99dbc7852e9fb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTORS: List[Dict[int, float]] = []\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    vector = {}\n",
    "    \n",
    "    # TF is 1 as dimensions will be idempotently overwritten\n",
    "    for token in tweet['tokens']:\n",
    "        vector[TOKEN_IDS[token]] = INVERSE_DOCUMENT_FREQUENCIES[token]\n",
    "        \n",
    "    # TODO: I need to make fog's sparse_normalize mutating\n",
    "    vector = sparse_normalize(vector)\n",
    "    VECTORS.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52584103-a605-4688-98b6-af7e5641e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1173: 0.3392219177138808,\n",
       " 1174: 0.27395917266349423,\n",
       " 433: 0.27801079648645943,\n",
       " 371: 0.32618577518271835,\n",
       " 611: 0.16106866655730037,\n",
       " 1175: 0.27360774231947405,\n",
       " 229: 0.07852205627881487,\n",
       " 1176: 0.30416522042764255,\n",
       " 1177: 0.3421793749637862,\n",
       " 1178: 0.3496124408813681,\n",
       " 1179: 0.2846150201930425,\n",
       " 1180: 0.25663325400546066,\n",
       " 102: 0.20129399510952986}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS[254]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003abe-493c-45d7-bed4-3b8ddb550dc1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "239f848a-fd52-485e-bb22-becfe33b1e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a577c1665547fbbc80ea8ac0d06fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clustering():\n",
    "    edges = []\n",
    "\n",
    "    for i, A in enumerate(VECTORS):\n",
    "        tweet_a = TWEETS[i]\n",
    "\n",
    "        date_bound = tweet_a['date'] + ONE_DAY\n",
    "        best_metric = None\n",
    "        best_candidate = None\n",
    "\n",
    "        for j in range(i + 1, len(VECTORS)):\n",
    "            tweet_b = TWEETS[j]\n",
    "\n",
    "            if tweet_b['date'] > date_bound:\n",
    "                break\n",
    "\n",
    "            B = VECTORS[j]\n",
    "\n",
    "            d = sparse_dot_product(A, B)\n",
    "\n",
    "            if d == 0.0:\n",
    "                continue\n",
    "\n",
    "            if best_candidate is None or d > best_metric:\n",
    "                best_metric = d\n",
    "                best_candidate = j\n",
    "\n",
    "        yield (i, best_candidate, best_metric)\n",
    "\n",
    "EDGES = []\n",
    "\n",
    "for edge in tqdm(clustering(), unit='tweet', total=len(VECTORS)):\n",
    "    if edge[1] is None:\n",
    "        continue\n",
    "    EDGES.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c19bc2e4-a349-4f9e-8dde-fe09a634e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar tweets (similarity: 0.327407):\n",
      "\n",
      "1025321398603005953 - Hey je suis mort j'ai r√™v√© que je me faisais friendzone par une meuf que mon cerveau a totalement invent√©. Vis ma vie d'√©ternel friendzon√©.  \n",
      "---\n",
      "1025443385526693888 - @roronoazon (√™tre tt le monde c sur fait ) wsh tu m‚Äôa friendzone ??  \n"
     ]
    }
   ],
   "source": [
    "similar_pair = choice(EDGES)\n",
    "first_tweet = TWEETS[similar_pair[0]]\n",
    "second_tweet = TWEETS[similar_pair[1]]\n",
    "print('Similar tweets (similarity: %f):\\n' % similar_pair[2])\n",
    "print(first_tweet['id'], '-', first_tweet['text+quote+reply'])\n",
    "print('---')\n",
    "print(second_tweet['id'], '-', second_tweet['text+quote+reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54b553fe-1ac4-4320-a6e5-6f08da779882",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH = nx.Graph()\n",
    "\n",
    "for i, j, sim in EDGES:\n",
    "    if sim < 0.7:\n",
    "        continue\n",
    "        \n",
    "    GRAPH.add_edge(TWEETS[i]['id'], TWEETS[j]['id'])\n",
    "\n",
    "CLUSTERS = list(nx.connected_components(GRAPH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6cecd-f38e-476f-ab8b-05a064722c60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f03141f-457b-4b8e-9b99-ee9523ee84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in labeled clusters: 18020\n",
      "Min number of tweets in labeled clusters: 2\n",
      "Mean number of tweets in labeled clusters: 372.74708171206225\n",
      "Median number of tweets in labeled clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9953ef19-c921-4fac-9ae6-740cd2ddf7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 10020\n",
      "Max number of tweets in labeled clusters: 1026\n",
      "Min number of tweets in labeled clusters: 2\n",
      "Mean number of tweets in labeled clusters: 3.9781437125748504\n",
      "Median number of tweets in labeled clusters: 2\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "549cb7a6-86a4-4f1e-87e4-64ed15b07b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9930137048385659, 0.02034559447955233, 0.03223855967971992)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_matching(TRUTH, CLUSTERS, allow_additional_items=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
