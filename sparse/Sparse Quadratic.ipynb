{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e001a-d9ad-4a85-80b9-03140f4cb361",
   "metadata": {},
   "source": [
    "# Sparse reproduction of clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a6196b-93ab-4ce7-b2c7-04f7bf073945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from datetime import datetime, timedelta\n",
    "from random import sample, choice\n",
    "from statistics import mean, median_low\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from fog.tokenizers import WordTokenizer\n",
    "from fog.metrics import sparse_normalize, sparse_dot_product\n",
    "from fog.evaluation import best_matching\n",
    "from twitwi.constants import TWEET_DATETIME_FORMAT\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b489f2-a483-4f77-8229-acb2c655707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001aa35-3247-4bfa-82d5-1e8b4c990453",
   "metadata": {},
   "source": [
    "## Constants and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59114cb3-07ff-48b8-a4c0-7dfb149bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def parse_date(created_at):\n",
    "    return datetime.strptime(created_at, TWEET_DATETIME_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f7ace-67f1-4afb-b02a-e4efc518be28",
   "metadata": {},
   "source": [
    "## Reading tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f768c4a4-8606-4b78-bf93-e0188735408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    ALL_TWEETS = list(csv.DictReader(f, delimiter='\\t'))\n",
    "    \n",
    "# Keeping tweets only once (to avoid fuzzy clusters present in the data)\n",
    "already_seen = set()\n",
    "TWEETS = []\n",
    "\n",
    "for tweet in ALL_TWEETS:\n",
    "    if tweet['id'] in already_seen:\n",
    "        continue\n",
    "    \n",
    "    already_seen.add(tweet['id'])\n",
    "    TWEETS.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263b7713-87a3-466a-9688-fa23ed49e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dates & parsing stuff\n",
    "for tweet in TWEETS:\n",
    "    tweet['event'] = int(tweet['event'])\n",
    "    tweet['date'] = parse_date(tweet['created_at'])\n",
    "    tweet['timestamp'] = tweet['date'].timestamp()\n",
    "    tweet['label'] = int(tweet['label'].split('.')[0]) if tweet['label'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984618b6-5c24-477a-9ed3-7a7aeed713a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making suuuuuuuure the tweets are sorted by date\n",
    "TWEETS = sorted(TWEETS, key=itemgetter('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce41a24-5cbf-4833-91ca-c2ff9b994463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1018722125941755905\n",
      "label_day 0.0\n",
      "event 20180716001\n",
      "text #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren\n",
      "text+quote+reply #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren  \n",
      "image \n",
      "url_image \n",
      "user1 True\n",
      "user2 True\n",
      "user3 True\n",
      "created_at Mon Jul 16 05:00:56 +0000 2018\n",
      "label 0\n",
      "date 2018-07-16 05:00:56\n",
      "timestamp 1531710056.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in TWEETS[0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfeef6e-7d15-41b6-ac37-53dafdf13ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 137757\n",
      "Total number of events: 327\n",
      "Total number of labels: 257\n",
      "Total number of tweets not labeled 41961\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets:', len(TWEETS))\n",
    "print('Total number of events:', len(set(t['event'] for t in TWEETS)))\n",
    "print('Total number of labels:', len(set(t['label'] for t in TWEETS if t['label'] is not None)))\n",
    "print('Total number of tweets not labeled', sum(1 if t['label'] is None else 0 for t in TWEETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357cfbfa-1562-46e8-ac54-5f659a88ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTH = defaultdict(list)\n",
    "\n",
    "for tweet in TWEETS:\n",
    "    if tweet['label'] is None:\n",
    "        continue\n",
    "        \n",
    "    TRUTH[tweet['label']].append(tweet['id'])\n",
    "\n",
    "TRUTH = list(TRUTH.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9ae949-8030-4c2d-b5b0-63db36f45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_stats(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print('Number of clusters:', len(clusters))\n",
    "    print('Max number of tweets in labeled clusters:', max(lens))\n",
    "    print('Min number of tweets in labeled clusters:', min(lens))\n",
    "    print('Mean number of tweets in labeled clusters:', mean(lens))\n",
    "    print('Median number of tweets in labeled clusters:', median_low(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbac5d84-85f5-4d25-a16d-7a9dd636af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in labeled clusters: 18020\n",
      "Min number of tweets in labeled clusters: 2\n",
      "Mean number of tweets in labeled clusters: 372.74708171206225\n",
      "Median number of tweets in labeled clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098701-1e48-4de1-9503-7b578bb80473",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "*NOTE: It might be useful to convert tokens to incremental ids to speed up hash computations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64233d86-0760-4bc2-9558-d70d87fade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "    keep=['word'],\n",
    "    lower=True,\n",
    "    unidecode=True,\n",
    "    split_hashtags=True,\n",
    "    stoplist=STOP_WORDS_FR + [t + \"'\" for t in STOP_WORDS_FR],\n",
    "    reduce_words=True,\n",
    "    decode_html_entities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c23b5b-ae46-42b2-8442-2093e966575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macron re√ßoit patronat et syndicats pour d√©miner le terrain social https://t.co/NJ6AT76xHn via @LCI  \n",
      "[('word', 'macron'), ('word', 'recoit'), ('word', 'patronat'), ('word', 'syndicats'), ('word', 'deminer'), ('word', 'terrain'), ('word', 'social')]\n",
      "\n",
      "@oceane_barbier9 Panique agoisse crise de t√©tanie au milieu d'un carrefour  @deborah_lsr Ques qui c‚Äôest passez ? üò±\n",
      "[('word', 'panique'), ('word', 'agoisse'), ('word', 'crise'), ('word', 'tetanie'), ('word', 'milieu'), ('word', 'carrefour'), ('word', 'ques'), ('word', 'passez')]\n",
      "\n",
      "Que les f√©ministes religieuses qui hurlent au racisme en France se penchent sur la question plut√¥t que de chouiner au parlement car la France refusela burqa... #CCIF #Lallab et autres Diallo et indigestes de la r√©publique #CulCulClan    https://t.co/YLxliy5mJ2  \n",
      "[('word', 'feministes'), ('word', 'religieuses'), ('word', 'hurlent'), ('word', 'racisme'), ('word', 'france'), ('word', 'penchent'), ('word', 'question'), ('word', 'plutot'), ('word', 'chouiner'), ('word', 'parlement'), ('word', 'car'), ('word', 'france'), ('word', 'refusela'), ('word', 'burqa'), ('word', 'ccif'), ('word', 'lallab'), ('word', 'diallo'), ('word', 'indigestes'), ('word', 'republique'), ('word', 'cul'), ('word', 'cul'), ('word', 'clan')]\n",
      "\n",
      "# EODE-TV/ FABRICE BEAUR (EXPERT EODE) : IRAK - DEPUIS 2003, QUELLE PLACE DANS LA G√âOPOLITIQUE MONDIALE ? ..  https://t.co/4F9Y5syrjg https://t.co/PLzZqWcSBe  \n",
      "[('word', 'eode-tv'), ('word', 'fabrice'), ('word', 'beaur'), ('word', 'expert'), ('word', 'eode'), ('word', 'irak'), ('word', 'place'), ('word', 'geopolitique'), ('word', 'mondiale')]\n",
      "\n",
      "Trump enregistr√© √† son insu par son avocat¬†: il discutait d‚Äôun paiement √† une playmate https://t.co/0G8ctjwQs1  \n",
      "[('word', 'trump'), ('word', 'enregistre'), ('word', 'insu'), ('word', 'avocat'), ('word', 'discutait'), ('word', 'paiement'), ('word', 'playmate')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_to_tokenize = sample(TWEETS, 5)\n",
    "\n",
    "for tweet in sample_to_tokenize:\n",
    "    print(tweet['text+quote+reply'])\n",
    "    print(list(tokenizer(tweet['text+quote+reply'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cbcfd1-3dcf-4138-a07d-ba386395893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601a9030a89b4086a24c86902ed34ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCIES = Counter()\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    tweet['tokens'] = [token for _, token in tokenizer(tweet['text+quote+reply'])]\n",
    "    for token in tweet['tokens']:\n",
    "        DOCUMENT_FREQUENCIES[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e87b55-0fe8-4050-a2c1-ea8708ba85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 95635\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary:', len(DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00369b8-515f-437c-9e53-cd64e97ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(DOCUMENT_FREQUENCIES)\n",
    "TOKEN_IDS = {}\n",
    "INVERSE_DOCUMENT_FREQUENCIES = {}\n",
    "\n",
    "for i, (token, df) in enumerate(DOCUMENT_FREQUENCIES.items()):\n",
    "    TOKEN_IDS[token] = i\n",
    "    INVERSE_DOCUMENT_FREQUENCIES[token] = 1 + math.log((N + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3bb566a-486f-46e8-843a-caf57ae39435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30cc44327db4e038d2739d936a91275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTORS: List[Dict[int, float]] = []\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    vector = {}\n",
    "    \n",
    "    # TF is 1 as dimensions will be idempotently overwritten\n",
    "    for token in tweet['tokens']:\n",
    "        vector[TOKEN_IDS[token]] = INVERSE_DOCUMENT_FREQUENCIES[token]\n",
    "        \n",
    "    # TODO: I need to make fog's sparse_normalize mutating\n",
    "    vector = sparse_normalize(vector)\n",
    "    VECTORS.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52584103-a605-4688-98b6-af7e5641e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1184: 0.3335362851343286,\n",
       " 1185: 0.2775319260089816,\n",
       " 435: 0.28102368503877057,\n",
       " 372: 0.32240638755340006,\n",
       " 614: 0.17995422843611114,\n",
       " 1186: 0.27722900180324805,\n",
       " 1187: 0.303528466173047,\n",
       " 1188: 0.336055150888726,\n",
       " 1189: 0.34237413480618273,\n",
       " 1190: 0.2867125870028366,\n",
       " 1191: 0.26258832347667904,\n",
       " 105: 0.21478027123325547}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS[254]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003abe-493c-45d7-bed4-3b8ddb550dc1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7450cda3-57bd-4e82-ae22-d386ce1df103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import cython\n",
    "from datetime import timedelta\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def sparse_dot_product(A: dict, B: dict):\n",
    "    \n",
    "    # Swapping so we iterate over the smallest set\n",
    "    if len(A) > len(B):\n",
    "        A, B = B, A\n",
    "\n",
    "    cdef float product  = 0.0\n",
    "\n",
    "    for k, w1 in A.items():\n",
    "        w2 = B.get(k)\n",
    "\n",
    "        if w2 is not None:\n",
    "            product += w1 * w2\n",
    "\n",
    "    return product\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def clustering(vectors: list, tweets: list):\n",
    "    cdef float best_metric\n",
    "    cdef int best_candidate\n",
    "    cdef float d\n",
    "    \n",
    "    cdef float one_day = timedelta(days=1).total_seconds()\n",
    "    cdef float date_bound\n",
    "    \n",
    "    for i, A in enumerate(vectors):\n",
    "        tweet_a = tweets[i]\n",
    "\n",
    "        date_bound = tweet_a['timestamp'] + one_day\n",
    "        best_metric = -1.0\n",
    "        best_candidate = -1\n",
    "        d = 0.0\n",
    "\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            tweet_b = tweets[j]\n",
    "\n",
    "            if tweet_b['timestamp'] > date_bound:\n",
    "                break\n",
    "\n",
    "            B = vectors[j]\n",
    "\n",
    "            d = sparse_dot_product(A, B)\n",
    "\n",
    "            if d == 0.0:\n",
    "                continue\n",
    "\n",
    "            if best_candidate < 0 or d > best_metric:\n",
    "                best_metric = d\n",
    "                best_candidate = j\n",
    "        \n",
    "        if best_candidate != -1:\n",
    "            yield (i, best_candidate, best_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239f848a-fd52-485e-bb22-becfe33b1e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8617208d8047daa0ba649ca3ffa712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EDGES = []\n",
    "\n",
    "for edge in tqdm(clustering(VECTORS, TWEETS), unit='tweet', total=len(VECTORS)):\n",
    "    if edge[1] is None:\n",
    "        continue\n",
    "    EDGES.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19bc2e4-a349-4f9e-8dde-fe09a634e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar tweets (similarity: 0.296550):\n",
      "\n",
      "1026538888649748480 - Pq qd je suis sur Twitter h24 y a r et la je quitte Twitter genre 5h ET BAM! On apprend que NORMAN DRAGUE DES MINEURS PR BZ Quel monde  \n",
      "---\n",
      "1026585127017697280 - comment ca Norman il baise des mineurs mdr mais ????  \n"
     ]
    }
   ],
   "source": [
    "similar_pair = choice(EDGES)\n",
    "first_tweet = TWEETS[similar_pair[0]]\n",
    "second_tweet = TWEETS[similar_pair[1]]\n",
    "print('Similar tweets (similarity: %f):\\n' % similar_pair[2])\n",
    "print(first_tweet['id'], '-', first_tweet['text+quote+reply'])\n",
    "print('---')\n",
    "print(second_tweet['id'], '-', second_tweet['text+quote+reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54b553fe-1ac4-4320-a6e5-6f08da779882",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH = nx.Graph()\n",
    "\n",
    "for i, j, sim in EDGES:\n",
    "    i_id = TWEETS[i]['id']\n",
    "    j_id = TWEETS[j]['id']\n",
    "    \n",
    "    GRAPH.add_node(i_id)\n",
    "    GRAPH.add_node(j_id)\n",
    "    \n",
    "    if sim < 0.08:\n",
    "        continue\n",
    "        \n",
    "    GRAPH.add_edge(i_id, j_id)\n",
    "\n",
    "CLUSTERS = list(nx.connected_components(GRAPH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6cecd-f38e-476f-ab8b-05a064722c60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f03141f-457b-4b8e-9b99-ee9523ee84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Max number of tweets in labeled clusters: 18020\n",
      "Min number of tweets in labeled clusters: 2\n",
      "Mean number of tweets in labeled clusters: 372.74708171206225\n",
      "Median number of tweets in labeled clusters: 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9953ef19-c921-4fac-9ae6-740cd2ddf7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 242\n",
      "Max number of tweets in labeled clusters: 63893\n",
      "Min number of tweets in labeled clusters: 1\n",
      "Mean number of tweets in labeled clusters: 568.400826446281\n",
      "Median number of tweets in labeled clusters: 1\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "549cb7a6-86a4-4f1e-87e4-64ed15b07b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8545864213920271, 0.12889976232091693, 0.12059140549742423)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_matching(TRUTH, CLUSTERS, allow_additional_items=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
