{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e001a-d9ad-4a85-80b9-03140f4cb361",
   "metadata": {},
   "source": [
    "# Sparse reproduction of clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1a6196b-93ab-4ce7-b2c7-04f7bf073945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from datetime import datetime, timedelta\n",
    "from random import sample, choice\n",
    "from statistics import mean, median_low\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from fog.tokenizers import WordTokenizer\n",
    "from fog.metrics import sparse_normalize, sparse_dot_product\n",
    "from fog.evaluation import best_matching, clusters_to_labels\n",
    "from twitwi.constants import TWEET_DATETIME_FORMAT\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from typing import List, Dict\n",
    "from ebbe import distinct, partitioned_items\n",
    "from sklearn.metrics import fowlkes_mallows_score, v_measure_score, homogeneity_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b489f2-a483-4f77-8229-acb2c655707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001aa35-3247-4bfa-82d5-1e8b4c990453",
   "metadata": {},
   "source": [
    "## Constants and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59114cb3-07ff-48b8-a4c0-7dfb149bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def parse_date(created_at):\n",
    "    return datetime.strptime(created_at, TWEET_DATETIME_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f7ace-67f1-4afb-b02a-e4efc518be28",
   "metadata": {},
   "source": [
    "## Reading tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f768c4a4-8606-4b78-bf93-e0188735408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    TWEETS = sorted(\n",
    "        distinct(csv.DictReader(f, delimiter='\\t'), key=itemgetter('id')),\n",
    "        key=itemgetter('id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263b7713-87a3-466a-9688-fa23ed49e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dates & parsing stuff\n",
    "for i, tweet in enumerate(TWEETS):\n",
    "    tweet['index'] = i\n",
    "    tweet['event'] = int(tweet['event'])\n",
    "    tweet['date'] = parse_date(tweet['created_at'])\n",
    "    tweet['timestamp'] = tweet['date'].timestamp()\n",
    "    tweet['label'] = int(tweet['label'].split('.')[0]) if tweet['label'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84eb06e5-db6e-4ce0-9f3b-8727d7aaedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_INDEX = {t['id']: t for t in TWEETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57927431-9a08-40c2-8f7d-90fe96fc357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6261.681818181818"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = Counter(t['date'].isoformat()[:10] for t in TWEETS)\n",
    "mean(days.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fce41a24-5cbf-4833-91ca-c2ff9b994463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1018722125941755905\n",
      "label_day 0.0\n",
      "event 20180716001\n",
      "text #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren\n",
      "text+quote+reply #Rennes - La sortie de prison de Djamel Beghal [Vid√©o exclusive] via @letelegramme https://t.co/tbOthY1Ren  \n",
      "image \n",
      "url_image \n",
      "user1 True\n",
      "user2 True\n",
      "user3 True\n",
      "created_at Mon Jul 16 05:00:56 +0000 2018\n",
      "label 0\n",
      "index 0\n",
      "date 2018-07-16 05:00:56\n",
      "timestamp 1531710056.0\n",
      "tokens {'prison', 'beghal', 'sortie', 'rennes', 'exclusive', 'djamel', 'video'}\n",
      "vector {0: 0.30285878002097777, 1: 0.37211911498911404, 2: 0.36502759751995767, 3: 0.4393103267425311, 4: 0.48437121633298197, 5: 0.3724233412992261, 6: 0.26504828453270446}\n"
     ]
    }
   ],
   "source": [
    "for k, v in TWEETS[0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfeef6e-7d15-41b6-ac37-53dafdf13ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 137757\n",
      "Total number of events: 327\n",
      "Total number of labels: 257\n",
      "Total number of tweets not labeled 41961\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets:', len(TWEETS))\n",
    "print('Total number of events:', len(set(t['event'] for t in TWEETS)))\n",
    "print('Total number of labels:', len(set(t['label'] for t in TWEETS if t['label'] is not None)))\n",
    "print('Total number of tweets not labeled', sum(1 if t['label'] is None else 0 for t in TWEETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357cfbfa-1562-46e8-ac54-5f659a88ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTH = partitioned_items((t['label'], t['id']) for t in TWEETS if t['label'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9ae949-8030-4c2d-b5b0-63db36f45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_stats(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print('Number of clusters:', len(clusters))\n",
    "    print('Number of non-singleton clusters:', sum(l > 1 for l in lens))\n",
    "    print('Max number of tweets in clusters:', max(lens))\n",
    "    print('Min number of tweets in clusters:', min(lens))\n",
    "    print('Mean number of tweets in clusters:', mean(lens))\n",
    "    print('Median number of tweets in clusters:', median_low(lens))\n",
    "    print('Median number of tweets in non-singleton clusters', median_low(l for l in lens if l > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbac5d84-85f5-4d25-a16d-7a9dd636af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Number of non-singleton clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n",
      "Median number of tweets in non-singleton clusters 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098701-1e48-4de1-9503-7b578bb80473",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "*NOTE: It might be useful to convert tokens to incremental ids to speed up hash computations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64233d86-0760-4bc2-9558-d70d87fade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "    keep=['word'],\n",
    "    lower=True,\n",
    "    unidecode=True,\n",
    "    split_hashtags=True,\n",
    "    stoplist=STOP_WORDS_FR + [t + \"'\" for t in STOP_WORDS_FR] + [t + \"‚Äô\" for t in STOP_WORDS_FR],\n",
    "    reduce_words=True,\n",
    "    decode_html_entities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c23b5b-ae46-42b2-8442-2093e966575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PoseTonGaulois #MandelaDay #Mandela100 #AIF2018 #lesnapoleons Quels emoji etes-vous? https://t.co/9uNNZxfxzS\n",
      "[('word', 'pose'), ('word', 'gaulois'), ('word', 'mandela'), ('word', 'day'), ('word', 'mandela'), ('word', 'aif'), ('word', 'lesnapoleons'), ('word', 'quels'), ('word', 'emoji')]\n",
      "\n",
      "allez faites peter vos plus belles photos de #LunarEclipse pour ceux qui ont pas eu la chance de la voir\n",
      "[('word', 'allez'), ('word', 'peter'), ('word', 'belles'), ('word', 'photos'), ('word', 'lunar'), ('word', 'eclipse'), ('word', 'chance')]\n",
      "\n",
      "Il s‚Äôest aper√ßu que Abdegrosnour √©tait carr√©ment √† chier je pense... on ne pourra malheureusement jamais le vendre... faites qu‚Äôun club fasse une proposition !! #TeamOM #SCPOM\n",
      "[('word', 'apercu'), ('word', 'abdegrosnour'), ('word', 'carrement'), ('word', 'chier'), ('word', 'pourra'), ('word', 'malheureusement'), ('word', 'vendre'), ('word', 'club'), ('word', 'fasse'), ('word', 'proposition'), ('word', 'team'), ('word', 'om'), ('word', 'scpom')]\n",
      "\n",
      "Suite au tweet de Squeezie, n'h√©sitez pas √† tweeter vos histoires avec le #BalanceTonYoutuber üçø\n",
      "[('word', 'suite'), ('word', 'tweet'), ('word', 'squeezie'), ('word', 'hesitez'), ('word', 'tweeter'), ('word', 'histoires'), ('word', 'balance'), ('word', 'youtuber')]\n",
      "\n",
      "La CFE-CGC #Renault signe l‚Äôaccord relatif au dialogue social !  https://t.co/0NykQqpfBt @gabfd https://t.co/kfHr5J6Np6\n",
      "[('word', 'cfe-cgc'), ('word', 'renault'), ('word', 'signe'), ('word', 'accord'), ('word', 'relatif'), ('word', 'dialogue'), ('word', 'social')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_to_tokenize = sample(TWEETS, 5)\n",
    "\n",
    "for tweet in sample_to_tokenize:\n",
    "    print(tweet['text'])\n",
    "    print(list(tokenizer(tweet['text'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cbcfd1-3dcf-4138-a07d-ba386395893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5666920786114a8498e4abc682a4b9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCIES = Counter()\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    tweet['tokens'] = set(token for _, token in tokenizer(tweet['text']))\n",
    "    for token in tweet['tokens']:\n",
    "        DOCUMENT_FREQUENCIES[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e87b55-0fe8-4050-a2c1-ea8708ba85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 84217\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary:', len(DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00369b8-515f-437c-9e53-cd64e97ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(TWEETS)\n",
    "TOKEN_IDS = {}\n",
    "INVERSE_DOCUMENT_FREQUENCIES = {}\n",
    "\n",
    "for i, (token, df) in enumerate(DOCUMENT_FREQUENCIES.items()):\n",
    "    if df < 10:\n",
    "        continue\n",
    "    TOKEN_IDS[token] = i\n",
    "    INVERSE_DOCUMENT_FREQUENCIES[token] = 1 + math.log((N + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbae9d4f-1b8e-4382-86fa-4b869e3958b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary after df trimming: 14221\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary after df trimming:', len(INVERSE_DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3bb566a-486f-46e8-843a-caf57ae39435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e0cb5b941e437fb589de5d91bda0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTORS: List[Dict[int, float]] = []\n",
    "\n",
    "for i, tweet in tqdm(enumerate(TWEETS), total=len(TWEETS)):\n",
    "    vector = {}\n",
    "\n",
    "    for token in tweet['tokens']:\n",
    "        idf = INVERSE_DOCUMENT_FREQUENCIES.get(token)\n",
    "        \n",
    "        if idf is None:\n",
    "            continue\n",
    "        \n",
    "        vector[TOKEN_IDS[token]] = idf\n",
    "        \n",
    "    # TODO: I need to make fog's sparse_normalize mutating\n",
    "    vector = sparse_normalize(vector)\n",
    "    VECTORS.append(vector)\n",
    "    TWEETS[i]['vector'] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52584103-a605-4688-98b6-af7e5641e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1053: 0.3420290070110478,\n",
       " 1054: 0.3270360552825462,\n",
       " 534: 0.19202726765211034,\n",
       " 1055: 0.26412400970698136,\n",
       " 314: 0.3115893902695856,\n",
       " 1056: 0.30280096548883645,\n",
       " 1057: 0.34877074198506636,\n",
       " 1058: 0.2771703452182757,\n",
       " 606: 0.22127831177409393,\n",
       " 1059: 0.2819053429277208,\n",
       " 1060: 0.2778883710559953,\n",
       " 375: 0.2766408301732054}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS[254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e3cb8e-cac6-40b7-aa2e-4f7c5a23e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997118113780062"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bool(v) for v in VECTORS) / len(VECTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003abe-493c-45d7-bed4-3b8ddb550dc1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5bbc1db-cdc5-421a-9c6d-fdc3bc4b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "def dot_product(A: dict, B: dict):\n",
    "    \n",
    "    # Swapping so we iterate over the smallest set\n",
    "    if len(A) > len(B):\n",
    "        A, B = B, A\n",
    "\n",
    "    cdef float product = 0.0\n",
    "\n",
    "    for k, w1 in A.items():\n",
    "        w2 = B.get(k)\n",
    "\n",
    "        if w2 is not None:\n",
    "            product += w1 * w2\n",
    "\n",
    "    return 1.0 - product\n",
    "\n",
    "def clustering(tweets, threshold=0.7):\n",
    "    best_candidate = None\n",
    "    cdef float best_distance\n",
    "    cdef int w = 6262\n",
    "    cdef int thread_id = -1\n",
    "    \n",
    "    threads = {}\n",
    "    \n",
    "    T = deque()\n",
    "    I = defaultdict(deque)\n",
    "    \n",
    "    for t1 in tweets:\n",
    "        \n",
    "        best_candidate = None\n",
    "        best_distance = 2.0\n",
    "        \n",
    "        C = set()\n",
    "        \n",
    "        for dim in t1['vector'].keys():\n",
    "            for c in I[dim]:\n",
    "                C.add(c)\n",
    "            I[dim].append(t1['index'])\n",
    "        \n",
    "        for c in C:\n",
    "            t2 = tweets[c]\n",
    "            d = dot_product(t1['vector'], t2['vector'])\n",
    "            \n",
    "            if d > threshold:\n",
    "                continue\n",
    "            \n",
    "            if d < best_distance:\n",
    "                best_distance = d\n",
    "                best_candidate = t2\n",
    "\n",
    "        if best_candidate is not None:\n",
    "            threads[t1['index']] = threads[best_candidate['index']]\n",
    "        else:\n",
    "            thread_id += 1\n",
    "            threads[t1['index']] = thread_id\n",
    "            \n",
    "        yield (t1['index'], threads[t1['index']])\n",
    "\n",
    "        T.append(t1)\n",
    "\n",
    "        if len(T) > w:\n",
    "            t3 = T.popleft()\n",
    "            \n",
    "            for dim in t3['vector'].keys():\n",
    "                I[dim].popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ca3f730-8309-412c-8c45-40c5dd4a79d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39cdaeae014e09963cdb694b799ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threads = []\n",
    "\n",
    "for i, thread_id in tqdm(clustering(TWEETS, 0.7), total=len(TWEETS)):\n",
    "    threads.append((i, thread_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2371341-865e-4a41-a16c-f48614b32796",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = partitioned_items((thread_id, TWEETS[i]['id']) for i, thread_id in threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6cecd-f38e-476f-ab8b-05a064722c60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec0b3d12-7500-4cec-9b6c-e72f804471d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_ids = set()\n",
    "truth_labels = {}\n",
    "truth_order = {}\n",
    "\n",
    "c = 0\n",
    "for i, cluster in enumerate(TRUTH):\n",
    "    for _id in cluster:\n",
    "        truth_ids.add(_id)\n",
    "        truth_labels[_id] = i\n",
    "        truth_order[_id] = c\n",
    "        c += 1\n",
    "\n",
    "predicted_labels = {}\n",
    "for i, cluster in enumerate(CLUSTERS):\n",
    "    for _id in cluster:\n",
    "        if _id not in truth_labels:\n",
    "            continue\n",
    "            \n",
    "        predicted_labels[_id] = i\n",
    "\n",
    "truth_labels = [v for _, v in sorted(truth_labels.items(), key=lambda t: truth_order[t[0]])]\n",
    "predicted_labels = [v for _, v in sorted(predicted_labels.items(), key=lambda t: truth_order[t[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f03141f-457b-4b8e-9b99-ee9523ee84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Number of non-singleton clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n",
      "Median number of tweets in non-singleton clusters 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f37d793-4ccd-401c-b99b-2d2325e9b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 29324\n",
      "Number of non-singleton clusters: 4844\n",
      "Max number of tweets in clusters: 3482\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 4.697756104214977\n",
      "Median number of tweets in clusters: 1\n",
      "Median number of tweets in non-singleton clusters 3\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28754ce0-de39-484a-be6b-52a14053e075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999131481454262, 0.7044670083171446, 0.6860234200601311)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best matching macro average\n",
    "best_matching(TRUTH, CLUSTERS, allow_additional_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca1fcd99-4eff-40f8-a4b4-cbb8de6f1529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4515286039188244, 0.43107227859200803, 0.441063379830389)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best matching micro average\n",
    "best_matching(TRUTH, CLUSTERS, allow_additional_items=True, micro=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec0c0343-4e15-4ec0-9091-3ba3e8530d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23379028402172444"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fowlkes-Mallows score\n",
    "fowlkes_mallows_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d969ab97-e9eb-4a9d-b802-167cb6be1751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892807009260605"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Homogeneity score\n",
    "homogeneity_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e11acea6-8af2-4111-a3e0-e001db51f766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.545779616570369"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completeness score\n",
    "completeness_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7163e975-787e-4ab1-a7c5-30dcf07ba1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774369487887107"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v-measure\n",
    "v_measure_score(truth_labels, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
