{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e001a-d9ad-4a85-80b9-03140f4cb361",
   "metadata": {},
   "source": [
    "# Sparse reproduction of clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a6196b-93ab-4ce7-b2c7-04f7bf073945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from datetime import datetime, timedelta\n",
    "from random import sample, choice\n",
    "from statistics import mean, median_low\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from fog.tokenizers import WordTokenizer\n",
    "from fog.metrics import sparse_normalize, sparse_dot_product\n",
    "from fog.evaluation import best_matching_macro_average, clusters_to_labels\n",
    "from twitwi.constants import TWEET_DATETIME_FORMAT\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from typing import List, Dict\n",
    "from ebbe import distinct, partitioned_items\n",
    "from sklearn.metrics import fowlkes_mallows_score, v_measure_score, homogeneity_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b489f2-a483-4f77-8229-acb2c655707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001aa35-3247-4bfa-82d5-1e8b4c990453",
   "metadata": {},
   "source": [
    "## Constants and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59114cb3-07ff-48b8-a4c0-7dfb149bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def parse_date(created_at):\n",
    "    return datetime.strptime(created_at, TWEET_DATETIME_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f7ace-67f1-4afb-b02a-e4efc518be28",
   "metadata": {},
   "source": [
    "## Reading tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f768c4a4-8606-4b78-bf93-e0188735408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    TWEETS = sorted(\n",
    "        distinct(csv.DictReader(f, delimiter='\\t'), key=itemgetter('id')),\n",
    "        key=itemgetter('id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af435fde-6171-4640-932f-8cc1bcb03994",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [t for t in TWEETS if t['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263b7713-87a3-466a-9688-fa23ed49e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dates & parsing stuff\n",
    "for i, tweet in enumerate(TWEETS):\n",
    "    tweet['index'] = i\n",
    "    tweet['event'] = int(tweet['event'])\n",
    "    tweet['date'] = parse_date(tweet['created_at'])\n",
    "    tweet['timestamp'] = tweet['date'].timestamp()\n",
    "    tweet['label'] = int(tweet['label'].split('.')[0]) if tweet['label'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84eb06e5-db6e-4ce0-9f3b-8727d7aaedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_INDEX = {t['id']: t for t in TWEETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57927431-9a08-40c2-8f7d-90fe96fc357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4354"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = Counter(t['date'].isoformat()[:10] for t in TWEETS)\n",
    "WINDOW = int(mean(days.values()))\n",
    "WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce41a24-5cbf-4833-91ca-c2ff9b994463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1018722125941755905\n",
      "label_day 0.0\n",
      "event 20180716001\n",
      "text #Rennes - La sortie de prison de Djamel Beghal [Vidéo exclusive] via @letelegramme https://t.co/tbOthY1Ren\n",
      "text+quote+reply #Rennes - La sortie de prison de Djamel Beghal [Vidéo exclusive] via @letelegramme https://t.co/tbOthY1Ren  \n",
      "image \n",
      "url_image \n",
      "user1 True\n",
      "user2 True\n",
      "user3 True\n",
      "created_at Mon Jul 16 05:00:56 +0000 2018\n",
      "label 0\n",
      "index 0\n",
      "date 2018-07-16 05:00:56\n",
      "timestamp 1531710056.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in TWEETS[0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfeef6e-7d15-41b6-ac37-53dafdf13ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 95796\n",
      "Total number of events: 327\n",
      "Total number of labels: 257\n",
      "Total number of tweets not labeled 0\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tweets:', len(TWEETS))\n",
    "print('Total number of events:', len(set(t['event'] for t in TWEETS)))\n",
    "print('Total number of labels:', len(set(t['label'] for t in TWEETS if t['label'] is not None)))\n",
    "print('Total number of tweets not labeled', sum(1 if t['label'] is None else 0 for t in TWEETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357cfbfa-1562-46e8-ac54-5f659a88ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUTH = partitioned_items((t['label'], t['id']) for t in TWEETS if t['label'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9ae949-8030-4c2d-b5b0-63db36f45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_stats(clusters):\n",
    "    lens = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print('Number of clusters:', len(clusters))\n",
    "    print('Number of non-singleton clusters:', sum(l > 1 for l in lens))\n",
    "    print('Max number of tweets in clusters:', max(lens))\n",
    "    print('Min number of tweets in clusters:', min(lens))\n",
    "    print('Mean number of tweets in clusters:', mean(lens))\n",
    "    print('Median number of tweets in clusters:', median_low(lens))\n",
    "    print('Median number of tweets in non-singleton clusters', median_low(l for l in lens if l > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbac5d84-85f5-4d25-a16d-7a9dd636af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Number of non-singleton clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n",
      "Median number of tweets in non-singleton clusters 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99098701-1e48-4de1-9503-7b578bb80473",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization\n",
    "\n",
    "*NOTE: It might be useful to convert tokens to incremental ids to speed up hash computations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64233d86-0760-4bc2-9558-d70d87fade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(\n",
    "    keep=['word'],\n",
    "    lower=True,\n",
    "    unidecode=True,\n",
    "    split_hashtags=True,\n",
    "    stoplist=STOP_WORDS_FR + [t + \"'\" for t in STOP_WORDS_FR] + [t + \"’\" for t in STOP_WORDS_FR],\n",
    "    reduce_words=True,\n",
    "    decode_html_entities=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c23b5b-ae46-42b2-8442-2093e966575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post: \"Puigdemont y las falsas expectativas\" https://t.co/SkPNH1RLkI\n",
      "[('word', 'new'), ('word', 'post'), ('word', 'puigdemont'), ('word', 'las'), ('word', 'falsas'), ('word', 'expectativas')]\n",
      "\n",
      "Avron -&gt; « Nous Avron Gagné » CDG – Etoile -&gt;  « On a 2 Étoiles » Victor Hugo -&gt; « Victor Hugo Lloris » Bercy -&gt; « Bercy les Bleus » Notre-Dame des Champs -&gt;« Notre Didier Deschamps » Champs Elysées-Clémenceau -&gt; « Deschamps Elysées – Clémenceau » #RATP\n",
      "[('word', 'avron'), ('word', 'avron'), ('word', 'gagne'), ('word', 'cdg'), ('word', 'etoile'), ('word', 'etoiles'), ('word', 'victor'), ('word', 'hugo'), ('word', 'victor'), ('word', 'hugo'), ('word', 'lloris'), ('word', 'bercy'), ('word', 'bercy'), ('word', 'bleus'), ('word', 'notre-dame'), ('word', 'champs'), ('word', 'didier'), ('word', 'deschamps'), ('word', 'champs'), ('word', 'elysees-clemenceau'), ('word', 'deschamps'), ('word', 'elysees'), ('word', 'clemenceau'), ('word', 'ratp')]\n",
      "\n",
      "#affairebenalla Ici Londres.. Ici Londres Le chocolat fond Je répète  Le chocolat fond https://t.co/qI8tn3Rhh4\n",
      "[('word', 'affairebenalla'), ('word', 'ici'), ('word', 'londres'), ('word', 'ici'), ('word', 'londres'), ('word', 'chocolat'), ('word', 'fond'), ('word', 'repete'), ('word', 'chocolat'), ('word', 'fond')]\n",
      "\n",
      "La #SNCF commande à Alstom une centaine de “TGV du futur” https://t.co/zqG8h7neNU\n",
      "[('word', 'sncf'), ('word', 'commande'), ('word', 'alstom'), ('word', 'centaine'), ('word', 'tgv'), ('word', 'futur')]\n",
      "\n",
      "@lauraleishman @TheDailyShow @GerardAraud Donc l'Islam se résume au hijab? Merci pour cette brillante intervention.\n",
      "[('word', 'islam'), ('word', 'resume'), ('word', 'hijab'), ('word', 'brillante'), ('word', 'intervention')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_to_tokenize = sample(TWEETS, 5)\n",
    "\n",
    "for tweet in sample_to_tokenize:\n",
    "    print(tweet['text'])\n",
    "    print(list(tokenizer(tweet['text'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cbcfd1-3dcf-4138-a07d-ba386395893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e83be71889b468d8ebdb164769642fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOCUMENT_FREQUENCIES = Counter()\n",
    "\n",
    "for tweet in tqdm(TWEETS):\n",
    "    tweet['tokens'] = set(token for _, token in tokenizer(tweet['text']))\n",
    "    for token in tweet['tokens']:\n",
    "        DOCUMENT_FREQUENCIES[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e87b55-0fe8-4050-a2c1-ea8708ba85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 57091\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary:', len(DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c00369b8-515f-437c-9e53-cd64e97ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(TWEETS)\n",
    "TOKEN_IDS = {}\n",
    "INVERSE_DOCUMENT_FREQUENCIES = {}\n",
    "\n",
    "for i, (token, df) in enumerate(DOCUMENT_FREQUENCIES.items()):\n",
    "    if df > 10:\n",
    "        TOKEN_IDS[token] = i\n",
    "        INVERSE_DOCUMENT_FREQUENCIES[token] = 1 + math.log((N + 1) / (df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbae9d4f-1b8e-4382-86fa-4b869e3958b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary after df trimming: 9704\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary after df trimming:', len(INVERSE_DOCUMENT_FREQUENCIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3bb566a-486f-46e8-843a-caf57ae39435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17379e6739e64b46ba5bc6bcbffcf363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VECTORS: List[Dict[int, float]] = []\n",
    "\n",
    "for i, tweet in tqdm(enumerate(TWEETS), total=len(TWEETS)):\n",
    "    vector = {}\n",
    "\n",
    "    for token in tweet['tokens']:\n",
    "        idf = INVERSE_DOCUMENT_FREQUENCIES.get(token)\n",
    "        \n",
    "        if idf is None:\n",
    "            continue\n",
    "        \n",
    "        vector[TOKEN_IDS[token]] = idf\n",
    "        \n",
    "    # TODO: I need to make fog's sparse_normalize mutating\n",
    "    vector = sparse_normalize(vector)\n",
    "    VECTORS.append(vector)\n",
    "    TWEETS[i]['vector'] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52584103-a605-4688-98b6-af7e5641e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{541: 0.2688992359918619,\n",
       " 542: 0.2774942754803345,\n",
       " 68: 0.2774942754803345,\n",
       " 85: 0.2747254653925353,\n",
       " 543: 0.321455114985555,\n",
       " 544: 0.28384648546562263,\n",
       " 174: 0.2498417554353074,\n",
       " 545: 0.19237199497632934,\n",
       " 546: 0.18464291051057577,\n",
       " 78: 0.2758052159443864,\n",
       " 547: 0.323727011709852,\n",
       " 548: 0.23632412414120496,\n",
       " 549: 0.25353371544288056,\n",
       " 550: 0.21442209835940496,\n",
       " 551: 0.18389370786038023}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS[254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46e3cb8e-cac6-40b7-aa2e-4f7c5a23e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967952732890726"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bool(v) for v in VECTORS) / len(VECTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e003abe-493c-45d7-bed4-3b8ddb550dc1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5bbc1db-cdc5-421a-9c6d-fdc3bc4b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from collections import deque, defaultdict\n",
    "from statistics import mean, median\n",
    "\n",
    "def dot_product(A: dict, B: dict):\n",
    "    \n",
    "    # Swapping so we iterate over the smallest set\n",
    "    if len(A) > len(B):\n",
    "        A, B = B, A\n",
    "\n",
    "    cdef float product = 0.0\n",
    "\n",
    "    for k, w1 in A.items():\n",
    "        w2 = B.get(k)\n",
    "\n",
    "        if w2 is not None:\n",
    "            product += w1 * w2\n",
    "\n",
    "    return 1.0 - product\n",
    "\n",
    "def clustering(tweets, window, threshold=0.7):\n",
    "    best_candidate = None\n",
    "    cdef float best_distance\n",
    "    cdef int thread_id = -1\n",
    "    \n",
    "    threads = {}\n",
    "    \n",
    "    T = deque()\n",
    "    I = defaultdict(deque)\n",
    "    M = []\n",
    "    \n",
    "    for t1 in tweets:\n",
    "        \n",
    "        best_candidate = None\n",
    "        best_distance = 2.0\n",
    "        \n",
    "        # TODO: index current tweet in sparse map\n",
    "        # TODO: it's possible to compute the cosine online when fetching candidates\n",
    "        \n",
    "        C = set()\n",
    "        \n",
    "        g = 0\n",
    "        for dim, _ in sorted(t1['vector'].items(), key=lambda x: (x[1], x[0]), reverse=True):\n",
    "            \n",
    "            if g < 256:\n",
    "                for c in I[dim]:\n",
    "                    C.add(c)\n",
    "            g += 1\n",
    "            I[dim].append(t1['index'])\n",
    "            \n",
    "        M.append(len(C))\n",
    "        \n",
    "        for c in C:\n",
    "            t2 = tweets[c]\n",
    "            d = dot_product(t1['vector'], t2['vector'])\n",
    "            \n",
    "            if d > threshold:\n",
    "                continue\n",
    "            \n",
    "            if d < best_distance:\n",
    "                best_distance = d\n",
    "                best_candidate = t2\n",
    "\n",
    "        if best_candidate is not None:\n",
    "            threads[t1['index']] = threads[best_candidate['index']]\n",
    "        else:\n",
    "            thread_id += 1\n",
    "            threads[t1['index']] = thread_id\n",
    "            \n",
    "        yield (t1['index'], threads[t1['index']])\n",
    "\n",
    "        T.append(t1)\n",
    "\n",
    "        if len(T) > window:\n",
    "            t3 = T.popleft()\n",
    "            \n",
    "            for dim in t3['vector'].keys():\n",
    "                I[dim].popleft()\n",
    "                \n",
    "    print(mean(M), median(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ca3f730-8309-412c-8c45-40c5dd4a79d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288220b4ae7749bd8344f8241a2f53d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581.371153284062 399.0\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "\n",
    "for i, thread_id in tqdm(clustering(TWEETS, WINDOW, 0.7), total=len(TWEETS)):\n",
    "    threads.append((i, thread_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2371341-865e-4a41-a16c-f48614b32796",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = partitioned_items((thread_id, TWEETS[i]['id']) for i, thread_id in threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6cecd-f38e-476f-ab8b-05a064722c60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec0b3d12-7500-4cec-9b6c-e72f804471d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_ids = set()\n",
    "truth_labels = {}\n",
    "truth_order = {}\n",
    "\n",
    "c = 0\n",
    "for i, cluster in enumerate(TRUTH):\n",
    "    for _id in cluster:\n",
    "        truth_ids.add(_id)\n",
    "        truth_labels[_id] = i\n",
    "        truth_order[_id] = TWEETS_INDEX[_id]['index']\n",
    "        c += 1\n",
    "\n",
    "predicted_labels = {}\n",
    "for i, cluster in enumerate(CLUSTERS):\n",
    "    for _id in cluster:\n",
    "        if _id not in truth_labels:\n",
    "            continue\n",
    "            \n",
    "        predicted_labels[_id] = i\n",
    "\n",
    "truth_labels = [v for _, v in sorted(truth_labels.items(), key=lambda t: truth_order[t[0]])]\n",
    "predicted_labels = [v for _, v in sorted(predicted_labels.items(), key=lambda t: truth_order[t[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f03141f-457b-4b8e-9b99-ee9523ee84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 257\n",
      "Number of non-singleton clusters: 257\n",
      "Max number of tweets in clusters: 18020\n",
      "Min number of tweets in clusters: 2\n",
      "Mean number of tweets in clusters: 372.74708171206225\n",
      "Median number of tweets in clusters: 76\n",
      "Median number of tweets in non-singleton clusters 76\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f37d793-4ccd-401c-b99b-2d2325e9b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 14398\n",
      "Number of non-singleton clusters: 2557\n",
      "Max number of tweets in clusters: 4859\n",
      "Min number of tweets in clusters: 1\n",
      "Mean number of tweets in clusters: 6.653424086678705\n",
      "Median number of tweets in clusters: 1\n",
      "Median number of tweets in non-singleton clusters 3\n"
     ]
    }
   ],
   "source": [
    "print_cluster_stats(CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28754ce0-de39-484a-be6b-52a14053e075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8798930674656514, 0.7099192593075736, 0.7363080016499431)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best matching macro average\n",
    "best_matching_macro_average(TRUTH, CLUSTERS, allow_additional_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec0c0343-4e15-4ec0-9091-3ba3e8530d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22241649983323722"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fowlkes-Mallows score\n",
    "fowlkes_mallows_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d969ab97-e9eb-4a9d-b802-167cb6be1751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8767583991120129"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Homogeneity score\n",
    "homogeneity_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11acea6-8af2-4111-a3e0-e001db51f766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5490804651924169"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completeness score\n",
    "completeness_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7163e975-787e-4ab1-a7c5-30dcf07ba1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6752669206848005"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v-measure\n",
    "v_measure_score(truth_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c987d-9502-48dd-a082-d34333867109",
   "metadata": {},
   "source": [
    "## Using original metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86c1775a-1f7d-42fe-8b90-b29f2f83a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5952c1d-b32e-4f6b-8ff1-cb3fa2249cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(TWEETS).sort_values(\"id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04a31a3b-e42b-4eb7-9f63-e5c2da3e2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_event_match(data, pred):\n",
    "    data[\"pred\"] = pd.Series(pred, dtype=data.label.dtype)\n",
    "\n",
    "    data = data[data.label.notna()]\n",
    "    print(\"{} labels, {} preds\".format(len(data.label.unique()), len(data.pred.unique())))\n",
    "    t0 = time.time()\n",
    "\n",
    "    match = data.groupby([\"label\", \"pred\"], sort=False).size().reset_index(name=\"a\")\n",
    "    b, c = [], []\n",
    "    for idx, row in match.iterrows():\n",
    "        b_ = ((data[\"label\"] != row[\"label\"]) & (data[\"pred\"] == row[\"pred\"]))\n",
    "        b.append(b_.sum())\n",
    "        c_ = ((data[\"label\"] == row[\"label\"]) & (data[\"pred\"] != row[\"pred\"]))\n",
    "        c.append(c_.sum())\n",
    "    print(\"match clusters with events took: {} seconds\".format(time.time() - t0))\n",
    "    match[\"b\"] = pd.Series(b)\n",
    "    match[\"c\"] = pd.Series(c)\n",
    "    # recall = nb true positive / (nb true positive + nb false negative)\n",
    "    match[\"r\"] = match[\"a\"] / (match[\"a\"] + match[\"c\"])\n",
    "    # precision = nb true positive / (nb true positive + nb false positive)\n",
    "    match[\"p\"] = match[\"a\"] / (match[\"a\"] + match[\"b\"])\n",
    "    match[\"f1\"] = 2 * match[\"r\"] * match[\"p\"] / (match[\"r\"] + match[\"p\"])\n",
    "    match = match.sort_values(\"f1\", ascending=False)\n",
    "    macro_average_f1 = match.drop_duplicates(\"label\").f1.mean()\n",
    "    macro_average_precision = match.drop_duplicates(\"label\").p.mean()\n",
    "    macro_average_recall = match.drop_duplicates(\"label\").r.mean()\n",
    "    return macro_average_precision, macro_average_recall, macro_average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cddb0930-b84b-465a-90bd-5ed00c5fd31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 labels, 14398 preds\n",
      "match clusters with events took: 25.84272599220276 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8798930674656512, 0.7099192593075742, 0.7363080016499435)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_event_match(data_df, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
