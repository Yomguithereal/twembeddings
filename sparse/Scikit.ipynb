{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f161cc-3d9f-4412-af5a-8eb17347eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from stop_words import STOP_WORDS_FR\n",
    "from ebbe import distinct\n",
    "from scipy import sparse\n",
    "from operator import itemgetter\n",
    "from sklearn.neighbors import NearestNeighbors, DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc6de55-4ddc-4427-aab8-a1325c486d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/event2018.tsv') as f:\n",
    "    TWEETS = sorted(\n",
    "        distinct(csv.DictReader(f, delimiter='\\t'), key=itemgetter('id')),\n",
    "        key=itemgetter('id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b41bdd3-f20b-48ab-860d-df38f168e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [t for t in TWEETS if t['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7f881d-4655-4ed0-87bd-9b5971efa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date_created_at(created_at):\n",
    "    if \"+0000\" in created_at:\n",
    "        d = datetime.strptime(created_at, TWITTER_DATE_FORMAT)\n",
    "    else:\n",
    "        d = datetime.strptime(created_at, STANDARD_DATE_FORMAT)\n",
    "    return d.strftime(\"%Y%m%d\"), d.strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def remove_repeted_characters(expr):\n",
    "    #limit number of repeted letters to 3. For example loooool --> loool\n",
    "    string_not_repeted = \"\"\n",
    "    for item in re.findall(r\"((.)\\2*)\", expr):\n",
    "        if len(item[0]) <= 3:\n",
    "            string_not_repeted += item[0]\n",
    "        else:\n",
    "            string_not_repeted += item[0][:3]\n",
    "    return string_not_repeted\n",
    "\n",
    "\n",
    "def camel_case_split(expr):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', expr)\n",
    "    return \" \".join([m.group(0) for m in matches])\n",
    "\n",
    "\n",
    "def format_text(text, **format):\n",
    "    # remove urls\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)\n",
    "    if format[\"remove_mentions\"]:\n",
    "        text = re.sub(r\"@\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # translate to equivalent ascii characters\n",
    "    if format[\"unidecode\"]:\n",
    "        text = unidecode(text)\n",
    "\n",
    "    new_text = []\n",
    "    for word in re.split(r\"[' ]\", text):\n",
    "        # remove numbers longer than 4 digits\n",
    "        if len(word) < 5 or not word.isdigit():\n",
    "            if word.startswith(\"#\") and format[\"hashtag_split\"]:\n",
    "                new_text.append(camel_case_split(word[1:]))\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "    text = remove_repeted_characters(\" \".join(new_text))\n",
    "    if format[\"lower\"]:\n",
    "        text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfe0a28-bfae-4b13-a4bb-cd42a96e1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = partial(format_text, remove_mentions=True, unidecode=True, lower=True, hashtag_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbd423f-18c8-4f38-a095-b096ca33ae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rennes - la sortie de prison de djamel beghal [video exclusive] via  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter('#Rennes - La sortie de prison de Djamel Beghal [VidÃ©o exclusive] via @letelegramme https://t.co/tbOthY1Ren')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f69bc8b-cb11-4b94-85f1-08a1b6180a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS_FR, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53be233b-0f51-4527-98de-9ceb3c632cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [formatter(t['text']) for t in TWEETS]\n",
    "vectorizer.fit(data)\n",
    "X = vectorizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c6b6e6-8d5e-4851-bdea-14a44098b0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55684"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ba0b94-2251-4911-821a-6d86b536af84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(X.sum(axis=0))[0,vectorizer.vocabulary_['letelegramme']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f5cc18b-5c9a-443a-9452-bfea9dd4750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X2 = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a1fc068-0f45-44fe-9ab6-0bd632d47139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.222393863862515"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_[vectorizer.vocabulary_['benalla']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "371c8a9f-b1e5-4020-83fe-3d4394756686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11217, 0.37215521880488034),\n",
       " (25768, 0.3724594746316988),\n",
       " (30739, 0.4844182110947467),\n",
       " (61170, 0.30288816404656704),\n",
       " (65496, 0.43935294959758503),\n",
       " (71481, 0.365063013301087),\n",
       " (79325, 0.2647077099615123)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = []\n",
    "for d in range(81987):\n",
    "    t = X2[0,d]\n",
    "    if t:\n",
    "        v.append((d, t))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "922a5451-fa9c-42a1-942d-68f70dfb4c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[79325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8705167c-4acb-495a-bd39-0ee23481df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.asarray(X.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943e6d7-7806-4514-8101-749939167111",
   "metadata": {},
   "source": [
    "other_transformer = TfidfTransformer()\n",
    "idf = np.log((len(TWEETS) + 1) / (df + 1)) + 1\n",
    "diag = sparse.diags(idf, offsets=0, shape=(len(df), len(df)), format=\"csr\", dtype=df.dtype)\n",
    "other_transformer._idf_diag = diag\n",
    "X3 = other_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cb9296a1-023c-4456-b7e8-43b1dd929764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.222393863862515"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_transformer.idf_[vectorizer.vocabulary_['benalla']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b1b66a49-280b-488b-8330-681c86806121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11217, 0.37215521880488034),\n",
       " (25768, 0.3724594746316988),\n",
       " (30739, 0.4844182110947467),\n",
       " (61170, 0.30288816404656704),\n",
       " (65496, 0.43935294959758503),\n",
       " (71481, 0.365063013301087),\n",
       " (79325, 0.2647077099615123)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = []\n",
    "for d in range(81987):\n",
    "    t = X3[0,d]\n",
    "    if t:\n",
    "        v.append((d, t))\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505e15d-499a-469a-8b89-2df4db563d67",
   "metadata": {},
   "source": [
    "## Direct solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727b85c6-c315-4909-a6df-d9565927bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_DATE_FORMAT = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "STANDARD_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def load_dataset(dataset, annotation, text=False):\n",
    "    data = pd.read_csv(dataset,\n",
    "                       sep=\"\\t\",\n",
    "                       quoting=csv.QUOTE_ALL,\n",
    "                       dtype={\"id\": str, \"label\": float, \"created_at\": str, \"text\": str}\n",
    "                       )\n",
    "    data.text = data.text.fillna(\"\")\n",
    "    if annotation == \"annotated\" and \"label\" in data.columns:\n",
    "        data = data[data.label.notna()]\n",
    "    elif annotation == \"examined\" and \"label\" in data.columns:\n",
    "        data = data[data.event.notna()]\n",
    "    if dataset == \"data/event2018_image\":\n",
    "        data = data[data.image.notna()]\n",
    "\n",
    "    if text == \"text+\" and \"text+quote+reply\" in data.columns:\n",
    "        data = data.rename(columns={\"text\": \"text_not_formated\", \"text+quote+reply\": \"text\"})\n",
    "    data[\"date\"], data[\"time\"] = zip(*data[\"created_at\"].apply(find_date_created_at))\n",
    "    return data.drop_duplicates(\"id\").sort_values(\"id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0ae35d-8add-4876-8aa7-079b7de7edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self, lang=\"fr\", binary=True):\n",
    "        self.df = np.array([])\n",
    "        self.features_names = []\n",
    "        self.n_samples = 0\n",
    "        self.name = \"tfidf\"\n",
    "        self.binary = binary\n",
    "        if lang == \"fr\":\n",
    "            self.stop_words = STOP_WORDS_FR\n",
    "        elif lang == \"en\":\n",
    "            self.stop_words = STOP_WORDS_EN\n",
    "\n",
    "    def load_history(self, lang):\n",
    "        if lang == \"fr\":\n",
    "            dataset = \"event2018\"\n",
    "        else:\n",
    "            dataset = \"event2012\"\n",
    "        for attr in [\"df\", \"features_names\", \"n_samples\"]:\n",
    "            with open(\"twembeddings/models/\" + dataset + \"_\" + attr, \"rb\") as f:\n",
    "                setattr(self, attr, pickle.load(f))\n",
    "        return self\n",
    "\n",
    "    def save(self, dataset):\n",
    "        dataset = dataset.split(\"/\")[-1].replace(\".tsv\", \"\")\n",
    "        for attr in [\"df\", \"features_names\", \"n_samples\"]:\n",
    "            with open(\"twembeddings/models/\" + dataset + \"_\" + attr, \"wb\") as f:\n",
    "                pickle.dump(getattr(self, attr), f)\n",
    "\n",
    "    def get_new_features(self, data):\n",
    "        features_set = set(self.features_names)\n",
    "        fit_model = CountVectorizer(stop_words=self.stop_words)\n",
    "        # see https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af for custom analyzr/tokenizr\n",
    "        fit_model.fit(data[\"text\"].tolist())\n",
    "        for term in fit_model.get_feature_names():\n",
    "            if term not in features_set:\n",
    "                self.features_names.append(term)\n",
    "\n",
    "    def build_count_vectors(self, data):\n",
    "        # sort words following features_name order, absent words will be counted as 0\n",
    "        count_model = CountVectorizer(binary=self.binary, vocabulary=self.features_names)\n",
    "        return count_model.transform(data[\"text\"].tolist())\n",
    "\n",
    "    def compute_df(self, count_vectors):\n",
    "        # add zeros to the end of the stored df vector\n",
    "        zeros = np.zeros(count_vectors.shape[1] - len(self.df), dtype=self.df.dtype)\n",
    "        df = np.append(self.df, zeros)\n",
    "        # compute new df array\n",
    "        # np.bincount counts each time an index is present in count_vectors.indices\n",
    "        # however it does not count \"zero\" for absent words\n",
    "        # therefore we artificially add all indices: np.arange(count_vectors.shape[1])\n",
    "        # and then substract 1 to all indices in the total score\n",
    "        indices = np.hstack((count_vectors.indices, np.arange(count_vectors.shape[1])))\n",
    "        df = df + np.bincount(indices) - 1\n",
    "        return df\n",
    "\n",
    "    def add_new_samples(self, data):\n",
    "        self.get_new_features(data)\n",
    "        count_vectors = self.build_count_vectors(data)\n",
    "        self.df = self.compute_df(count_vectors)\n",
    "        # logging.info(\"Count matrix shape: {}\".format(count_vectors.shape))\n",
    "        return count_vectors\n",
    "\n",
    "    def compute_vectors(self, count_vectors, min_df, svd=False, n_components=0):\n",
    "        if min_df > 0:\n",
    "            mask = self.df > min_df\n",
    "            df = self.df[mask]\n",
    "            count_vectors = count_vectors[:,mask]\n",
    "        else:\n",
    "            df = self.df\n",
    "        self.n_samples += count_vectors.shape[0]\n",
    "        # logging.info(\"Min_df reduces nb of features, new count matrix shape: {}\".format(\n",
    "        #     count_vectors.shape)\n",
    "        # )\n",
    "        # compute smoothed idf\n",
    "        idf = np.log((self.n_samples + 1) / (df + 1)) + 1\n",
    "        transformer = TfidfTransformer()\n",
    "        transformer._idf_diag = sparse.diags(idf, offsets=0, shape=(len(df), len(df)), format=\"csr\", dtype=df.dtype)\n",
    "        X = transformer.transform(count_vectors)\n",
    "        # equivalent to:\n",
    "        # X = normalize(X * transformer._idf_diag, norm='l2', copy=False)\n",
    "        if svd:\n",
    "            logging.info(\"Performing dimensionality reduction using LSA\")\n",
    "            svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "            normalizer = Normalizer(copy=False)\n",
    "            lsa = make_pipeline(svd, normalizer)\n",
    "            X = lsa.fit_transform(X)\n",
    "            logging.info(\"New shape: {}\".format(X.shape))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380f4d23-b1f5-4f02-8eb7-65db58b36a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('../data/event2018.tsv', 'annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383ba7be-7e2b-486f-9b61-de0ac83d4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "veco = TfIdf(lang='fr', binary=True)\n",
    "data.text = data.text.apply(format_text,\n",
    "                            remove_mentions=True,\n",
    "                            unidecode=True,\n",
    "                            lower=True,\n",
    "                            hashtag_split=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71a3dc77-925f-47f3-b468-51b0dbeecdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = veco.add_new_samples(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c78cec26-0c3f-4496-b3fb-482f52ad8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = veco.compute_vectors(count_matrix, min_df=10, svd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e0afbc-21d0-4999-a5fe-1a2bb258f409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95796x9803 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 870508 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c02ef690-e9a4-4b8b-9be4-3a192a8e940a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1701, 0.37215521880488034),\n",
       " (4218, 0.3724594746316988),\n",
       " (5045, 0.4844182110947467),\n",
       " (10017, 0.30288816404656704),\n",
       " (10814, 0.43935294959758503),\n",
       " (11872, 0.365063013301087),\n",
       " (13104, 0.2647077099615123)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = []\n",
    "for d in range(13486):\n",
    "    t = XX[0,d]\n",
    "    if t:\n",
    "        v.append((d, t))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8eb61bc-841d-4529-b516-f771cbc5a6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81., 637.,   4., ...,   1.,   4.,   1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veco.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d89c7f6-c355-4dc5-8c86-df557a8241c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95796"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veco.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74268835-d3d4-47da-8b90-fd6b63a45a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12520.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veco.df[veco.features_names.index('benalla')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dfbc798e-0bc6-4132-86cb-0848ff6d4af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14925.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veco.df[11385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "65122208-54c4-4efe-b8ca-e0496c47698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distances(x, y, intel_mkl=False):\n",
    "    x_normalized = normalize(x, copy=True)\n",
    "    y_normalized = normalize(y, copy=True)\n",
    "    if intel_mkl:\n",
    "        # s = dot_product_mkl(x_normalized, y_normalized.T.tocsr(), dense=True)\n",
    "        pass\n",
    "    else:\n",
    "        s = (x_normalized * y_normalized.T).toarray()\n",
    "    s *= -1\n",
    "    s += 1\n",
    "    np.clip(s, 0, 2, out=s)\n",
    "    if x is y or y is None:\n",
    "        # Ensure that distances between vectors and themselves are set to 0.0.\n",
    "        # This may not be the case due to floating point rounding errors.\n",
    "        s[np.diag_indices_from(s)] = 0.0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6ae1eecb-3c31-4e35-9a64-ed6cf1c0fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = cosine_distances(XX[:10], XX[10:30])\n",
    "neighbors = distances.argmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3225fb14-d38f-48a2-b8b8-0b9485ecad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.54289531, 0.56380689, 0.68234967, 1.        , 0.61963932,\n",
       "        0.        , 0.85527133, 0.90336077, 0.88501953, 0.0980216 ]),\n",
       " array([ 3, 15, 19,  0, 15, 12, 15,  7, 15,  9]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[range(distances.shape[0]), neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b6249db5-00d8-4e8d-ba85-511398b5b24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('algerie : le taux de natalite, parmi les plus eleves au monde, inquiete les autorites  via ',\n",
       " 'parmi les plus eleves au monde, le taux de natalite en algerie inquiete les autorites ')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[5]['text'], data.iloc[22]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
